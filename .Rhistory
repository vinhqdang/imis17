}
cluster <- replace(cluster, (cluster == i), xsub)
}
if (k != 0) stop("mergeResult: assertion failed (k = 0)...")
dimnames(centers) <- list(1:kk, NULL)
list(cluster = cluster, centers = centers, lnL = lnL, detVx = detVx, size = size)
}
# update the cluster number by using the result of "split2cls()"
# continue: no splitting
# v: cluster numbers vector for initial cluster.
# k1: cluster numbers should be updated; "k1" becomes "k1" and "k2"
# xsub: sub-cluster numbers vector of "v" whose value is "k1";
#	given "xsub" have 1 or 2.
updtCrusterNum <- function(continue, v, k1, k2, xsub){
if (!is.vector(v))
return(xsub)
if (!continue)
return(v)
if (k1 == k2)
stop("updtCrusterNum() : k1 and k2 should differ.")
# below is same algorithm; explicit array operation is slow in R.
# j <- 1
# for (i in 1:length(v)){
#	if (v[i] == k1){
#		if (xsub[j] == 2)
#			v[i] <- k2
#		j <- j + 1
#	}
# }
# end of algorithm
xsub <- replace(xsub, (xsub == 2), k2) # changed
xsub <- replace(xsub, (xsub == 1), k1) # unchanged
v <- replace(v, (v == k1), xsub)
}
# update the cluster centers by using the result of "split2cls()"
# continue: no update
# org.centers: original centers matrix
# divided.centers: divided centers matrix; it has 2 rows.
updtCenters <- function(continue, org.centers, k1, k2, divided.centers){
if (!is.matrix(org.centers))
return(divided.centers)
if (!continue)
return(org.centers)
if (k1 == k2)
stop("updtCenters() : k1 and k2 should differ.")
z <- NULL
for (i in 1:max(k2, nrow(org.centers))){
if (i == k1)
z <- rbind(z, divided.centers[1,])
else if (i == k2)
z <- rbind(z, divided.centers[2,])
else
z <- rbind(z, org.centers[i,])
}
z
}
# update the lnL by using the result of "split2cls()"
# continue: no update
# org.lnL: original lnL vector
# divided.lnL: divided lnL vector having 2 elements.
updtlnL <- function(continue, org.lnL, k1, k2, divided.lnL){
if (!is.vector(org.lnL))
return(divided.lnL)
if (!continue)
return(org.lnL)
if (k1 == k2)
stop("updtlnL() : k1 and k2 should differ.")
z <- NULL
for (i in 1:max(k2, length(org.lnL))){
if (i == k1)
z <- c(z, divided.lnL[1])
else if (i == k2)
z <- c(z, divided.lnL[2])
else
z <- c(z, org.lnL[i])
}
z
}
# update the detVx by using the result of "split2cls()"
# continue: no update
# org.detVx: original detVx vector
# divided.detVx: divided detVx vector having 2 elements.
updtdetVx <- function(continue, org.detVx, k1, k2, divided.detVx){
if (!is.vector(org.detVx))
return(divided.detVx)
if (!continue)
return(org.detVx)
if (k1 == k2)
stop("updtdetVx() : k1 and k2 should differ.")
z <- NULL
for (i in 1:max(k2, length(org.detVx))){
if (i == k1)
z <- c(z, divided.detVx[1])
else if (i == k2)
z <- c(z, divided.detVx[2])
else
z <- c(z, org.detVx[i])
}
z
}
# split 2 clusters if we would prefer it based on BIC
# q: a number of parameters
# bic.prior: BIC which x is given; if bic.prior=NULL then we calculate
# lnL.prior: lnL which x is given; if bic.prior=NULL then we calculate
# detVx.prior: detVx which x is given; if bic.prior=NULL then we calculate
split2cls <- function(x, centers, q, bic.prior, lnL.prior, detVx.prior, iter.max, ignore.covar){
if (is.null(bic.prior)){
pb <- priorBIC(x, centers, q, ignore.covar)
bic.prior <- pb$bic
lnL.prior <- pb$lnL
detVx.prior <- pb$detVx
}
bic.post <- postBICs(x, centers, q, iter.max, ignore.covar)
subcluster <- bic.post$clsub$cluster
#
# compare whether if we should split
if (is.na(bic.post$bic[3])){
# BIC may has NA because of few data
continue <- FALSE
}else if (bic.post$bic[3] < bic.prior){
# splitting ...
# replace the cluster number to cl$cluster
continue <- TRUE
}else{
# not splitting...
# return "subcluster" stored k1
continue <- FALSE
}
# note that "subcluster" gives 1 or 2
list(continue = continue, subcluster = subcluster,
bic.prior = bic.prior, bic.post = bic.post$bic,
lnL.prior = lnL.prior, lnL.post = bic.post$lnL,
detVx.prior = detVx.prior, detVx.post = bic.post$detVx,
centers = bic.post$clsub$centers,
clj1 = bic.post$clj1, clj2 = bic.post$clj2)
}
# return BIC (prior BIC)
priorBIC <- function(x, centers, q, ignore.covar){
lnL0 <- lnL(x, centers, ignore.covar)
bic <- -2 * lnL0$lnL + q * log(nrow(x)) # BIC
# bic <- -2 * lnL0$lnL + q  # AIC
list(lnL = lnL0$lnL, detVx = lnL0$detVx, bic = bic)
}
# return BICs (two posterior BICs)
postBICs <- function(x, centers, q, iter.max, ignore.covar){
#
# split to 2 clusters
clsub <- kmeans(x, 2, iter.max)
y.ok1 <- lapply(clsub$cluster, "==", 1) # 1st sub-cluster or not
y.ok2 <- lapply(clsub$cluster, "==", 2) # 2nd sub-cluster or not
# extract sub data
p <- ncol(x)
clj1 <- matrix(x[as.logical(y.ok1)], ncol=p)
clj2 <- matrix(x[as.logical(y.ok2)], ncol=p)
# ratio for pdf.
r1 <- clsub$size[1] / sum(clsub$size)	# [0,1]
r2 <- 1 - r1 	# [0,1]
# two later BICs
# print(clsub$centers[1,])	# for debug
# print(apply(clj1,2,mean))	# for debug
# print(sqrt(apply(clj1,2,var)))	# for debug
# print(r1)	# for debug
lnL1 <-  lnL(clj1, clsub$centers[1,], ignore.covar)
# print(clsub$centers[2,])	# for debug
# print(apply(clj2,2,mean))	# for debug
# print(sqrt(apply(clj2,2,var)))	# for debug
# print(r2)	# for debug
lnL2 <-  lnL(clj2, clsub$centers[2,], ignore.covar)
n1 <- nrow(clj1)
n2 <- nrow(clj2)
# normalizing factor; dist() is in library(mva)
if (is.na(lnL1$detVx) || is.na(lnL2$detVx))
beta <- 0
else
beta <- dist(clsub$center) / (sqrt(lnL1$detVx + lnL2$detVx))
alpha <- 0.5 / pnorm(beta)
BIC1 <- -2 * lnL1$lnL +q * log(n1)
BIC2 <- -2 * lnL2$lnL +q * log(n2)
# BIC1 <- -2 * lnL1$lnL +q # AIC
# BIC2 <- -2 * lnL2$lnL +q # AIC
# cat (paste("alpha =",alpha,"\n"))	# for debug
# cat (paste("beta =",beta,"\n"))	# for debug
# BIC is not (BIC1 + BIC2)
BIC <- -2 * lnL1$lnL  -2 * lnL2$lnL + 2 * q * log(n1 + n2) - 2 * (n1 + n2) * log(alpha)
# BIC <- -2 * lnL1$lnL  -2 * lnL2$lnL + 2 * q  - 2 * (n1 + n2) * log(alpha) # AIC
list(bic = c(BIC1, BIC2, BIC),
lnL = c(lnL1$lnL, lnL2$lnL),
detVx = c(lnL1$detVx, lnL2$detVx),
clsub = clsub, clj1 = clj1, clj2 = clj2)
}
# return BICs for Two-merged clusters model and devided clusters model
# k1/k2: marged cluster ID
mergedBIC <- function(x, xcl, k1, k2, q, ignore.covar, pr.proc){
# sample size
# check for input data
n1 <- xcl$size[k1]
n2 <- xcl$size[k2]
if (n1 == 0 || n2 == 0){
# already had been merged
cat(paste("already had been merged\n"))
ret <- F
return( list (ret = ret))
}
if (is.null(xcl$lnL[k1]) || is.null(xcl$lnL[k2])){
# lnL may be null because of few data
cat(paste("lnL may be null because of few data\n"))
ret <- F
return( list (ret = ret))
}
# divided clusters model
lnL1 = xcl$lnL[k1]
lnL2 = xcl$lnL[k2]
ctrextrt <- rbind(xcl$centers[k1,], xcl$centers[k2,])
beta <- dist(ctrextrt) / (sqrt(xcl$detVx[k1] + xcl$detVx[k2]))
if (pr.proc) cat(paste("beta=", round (beta, digit=2), "\n"))
# if (beta > 10){
# 	# 2 clusters far apart
# 	ret <- F
# 	return( list (ret = ret))
# }
alpha <- 0.5 / as.numeric(pnorm(beta))
bicdiv <- -2 * lnL1  -2 * lnL2 + 2 * q * log(n1 + n2) - 2 * (n1 + n2) * log(alpha)
# bicdiv <- -2 * lnL1 -2 * lnL2 + 2 * q - 2 * (n1 + n2) * log(alpha) # AIC
# extract 2 clusters data
y.ok1 <- lapply(xcl$cluster, "==", k1) # 1st sub-cluster or not
y.ok2 <- lapply(xcl$cluster, "==", k2) # 2nd sub-cluster or not
# extract sub data
p = ncol(x)
clj1 <- matrix(x[as.logical(y.ok1)], ncol=p)
clj2 <- matrix(x[as.logical(y.ok2)], ncol=p)
xmgd <- rbind(clj1, clj2)
# merged cluster center
ctrmgd <- (n1 * xcl$centers[k1,] + n2 * xcl$centers[k2,]) / (n1 + n2)
lnLmgd <- lnL(xmgd, ctrmgd, ignore.covar)
bicmgd <- -2 * lnLmgd$lnL + q * log(nrow(xmgd)) # BIC
# bicmgd <- -2 * lnLmgd$lnL + q  # AIC
ret <- T
list (ret = ret, ctrmgd = ctrmgd, lnLmgd = lnLmgd$lnL, detVxmgd = lnLmgd$detVx, bicmgd = bicmgd, bicdiv = bicdiv)
}
# log-likelihood under the assumption of
# p-dimensional multivariate normal distribution.
# ignore.covar: ignore the covariance
lnL <- function(x, centers, ignore.covar=T){
x <- as.matrix(x)
p <- ncol(x)	# p-dimensional multivariate
n <- nrow(x)	# sample size
if (missing(centers))
stop("centers must be a number or a matrix")
if (n <= 2)	# few data
return(list(lnL=NA, detVx=NA))
vx <- var(x)	# var-co.var matrix
# print(x)	# for debug
if (p == 1){ # x is vector
invVx <- 1 / as.vector(vx)
detVx <- as.vector(vx)
}else{
if (ignore.covar){
invVx <- diag(1/diag(vx)) # inv. matrix when assuming diag.
detVx <- prod(diag(vx)) # det. when assuming diag.
}else{
invVx <- solve(vx) # inverse matrix of "vx"
y <- chol(vx) # Cholesky decomposition
detVx <- prod(diag(y)) # vx = t(y) %*% y, where y is triangular,
# then, det(vx) = det(t(y)) * det(y)
}
}
t1 <- -p/2 * 1.837877066 # 1.837... = log(2 * 3.1415...)
t2 <- -log(detVx) / 2
xmu <- t(apply(x, 1, "-", centers))
# print(centers)	# for debug
# print(xmu)	# for debug
# s <- 0
# for (i in 1:n)
#	s <- s + t(xmu[i,]) %*% invVx %*% xmu[i,]
if (p == 1){
s <- sum(xmu^2 * invVx)
}else{
s <- sum(apply(xmu, 1, txInvVxX, invVx=invVx))
}
t3 <- -s / 2
ll <- (t1 + t2) * n + as.numeric(t3)	# log likelihood
list(lnL=ll, detVx=detVx)
}
# function for calculation of
# t(xmu[i,]) %*% invVx %*% xmu[i,]
txInvVxX <- function(x, invVx){
t(x) %*% invVx %*% x
}
xmeans(x = rnorm(50, sd=0.3), ik = 4)
matrix(rexp(200), 10)
size <- 20             #length of random number vectors
set.seed(1)
x <- runif(size)          # generate samples from uniform distribution (0.0, 1.0)
y <-runif(size)
df <-data.frame(x,y)
df
xmeans (df, ik=4)
plot (df)
size <- 200             #length of random number vectors
set.seed(1)
x <- runif(size)          # generate samples from uniform distribution (0.0, 1.0)
y <-runif(size)
df <-data.frame(x,y)
xmeans (df, ik=4)
plot (df)
y = xmeans (df, ik=4)
y$cluster
plot (df)
setwd("~/Desktop/API_VNM_DS2_en_csv_v2/")
data = read.csv("API_VNM_DS2_en_csv_v2.csv")
x = 1684
t = 1.05
x*t^10
x*t^50
x*t^100
x*t^50
x*t^60
x*t^70
x*t^65
x*t^65
setwd("~/workspace/imis17")
df = read.csv ("ismis17_trainingData/trainingData.csv", sep = ";")
test_df = read.csv ("testData.csv", sep=";")
library(gsubfn)
predicts = c()
# load the train data
Buys = c()
Holds = c()
Sells = c()
for (i in 1:nrow(df)) {
print (paste ("Processing line ", i))
x = df[i,]$Recommendation
x = as.character(x)
x1 = strapplyc (x, "\\{.*?\\}")[[1]]
cur_buy = 0
cur_hold = 0
cur_sell = 0
for (j in 1:length(x1)) {
x2 = substring(x1[[j]],2)
x3 = substr(x2, 1, nchar(x2) - 1)
recommendation = strsplit(x3, split = ",")
expert_idea = recommendation[[1]][2]
if (expert_idea == "Hold") {
cur_hold = cur_hold + 1
}
else if (expert_idea == "Buy") {
cur_buy = cur_buy + 1
}
else if (expert_idea == "Sell") {
cur_sell = cur_sell + 1
}
}
if (cur_sell + cur_buy + cur_hold != length(x1)) {
print (cur_buy)
print (cur_hold)
print (cur_sell)
print (length(x1))
waitKey()
}
Buys = c(Buys, cur_buy)
Sells = c(Sells, cur_sell)
Holds = c(Holds, cur_hold)
}
Result = df$Decision
df1 = data.frame(Buys, Holds, Sells, Result)
# load the test data
tBuys = c()
tHolds = c()
tSells = c()
for (i in 1:nrow(test_df)) {
print (paste ("Processing test data - line ", i))
x = test_df[i,]$Recommendation
x = as.character(x)
x1 = strapplyc (x, "\\{.*?\\}")[[1]]
cur_buy = 0
cur_hold = 0
cur_sell = 0
for (j in 1:length(x1)) {
x2 = substring(x1[[j]],2)
x3 = substr(x2, 1, nchar(x2) - 1)
recommendation = strsplit(x3, split = ",")
expert_idea = recommendation[[1]][2]
if (expert_idea == "Hold") {
cur_hold = cur_hold + 1
}
else if (expert_idea == "Buy") {
cur_buy = cur_buy + 1
}
else if (expert_idea == "Sell") {
cur_sell = cur_sell + 1
}
}
if (cur_sell + cur_buy + cur_hold != length(x1)) {
print (cur_buy)
print (cur_hold)
print (cur_sell)
print (length(x1))
waitKey()
}
tBuys = c(tBuys, cur_buy)
tSells = c(tSells, cur_sell)
tHolds = c(tHolds, cur_hold)
}
df2 = data.frame(tBuys, tHolds, tSells)
colnames (df2) = c("Buys","Holds","Sells")
library(h2o)
h2o.init()
predictors = 1:3
response = 4
nfolds = 5
hyper_params = list(
## restrict the search to the range of max_depth established above
max_depth = seq(20,30,1),
## search a large space of row sampling rates per tree
sample_rate = seq(0.2,1,0.01),
## search a large space of column sampling rates per split
col_sample_rate = seq(0.2,1,0.01),
## search a large space of column sampling rates per tree
col_sample_rate_per_tree = seq(0.2,1,0.01),
## search a large space of how column sampling per split should change as a function of the depth of the split
col_sample_rate_change_per_level = seq(0.9,1.1,0.01),
## search a large space of the number of min rows in a terminal node
min_rows = 2^seq(0,log2(nrow(df1))-1,1),
## search a large space of the number of bins for split-finding for continuous and integer columns
nbins = 2^seq(4,10,1),
## search a large space of the number of bins for split-finding for categorical columns
nbins_cats = 2^seq(4,12,1),
## search a few minimum required relative error improvement thresholds for a split to happen
min_split_improvement = c(0,1e-8,1e-6,1e-4),
## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
histogram_type = c("UniformAdaptive","QuantilesGlobal","RoundRobin")
)
search_criteria = list(
## Random grid search
strategy = "RandomDiscrete",
## limit the runtime to 60 minutes
max_runtime_secs = 3600,
## build no more than 100 models
max_models = 100,
## random number generator seed to make sampling of parameter combinations reproducible
seed = 1234,
## early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference
stopping_rounds = 5,
stopping_metric = "misclassification",
stopping_tolerance = 1e-3
)
grid <- h2o.grid(
## hyper parameters
hyper_params = hyper_params,
## hyper-parameter search configuration (see above)
search_criteria = search_criteria,
## which algorithm to run
algorithm = "gbm",
## identifier for the grid, to later retrieve it
grid_id = "final_grid",
## standard model parameters
x = predictors,
y = response,
training_frame = as.h2o(df1),
nfolds=5,
## more trees is better if the learning rate is small enough
## use "more than enough" trees - we have early stopping
ntrees = 10000,
## smaller learning rate is better
## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
## (use 1.00 to disable, but then lower the learning_rate)
learn_rate_annealing = 0.99,
## early stopping based on timeout (no model should take more than 1 hour - modify as needed)
max_runtime_secs = 3600,
## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
stopping_rounds = 5, stopping_tolerance = 1e-4, stopping_metric = "misclassification",
## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
score_tree_interval = 10,
## base random number generator seed for each model (automatically gets incremented internally for each model)
seed = 1234
)
## Sort the grid models by AUC
sortedGrid <- h2o.getGrid("final_grid", sort_by = "accuracy", decreasing = TRUE)
sortedGrid
str(df1)
gbm1 = h2o.getModel(sortedGrid[[1]])
gbm1 = h2o.getModel(sortedGrid@model_ids[[1]])
p2 = h2o.predict(gbm1, newdata = as.h2o (df2))
p2
p2 = as.vector (p2$predict)
p2
write.table(p2, file = "predic2.csv", col.names = FALSE, row.names = FALSE)
dnn1 = h2o.deeplearning(x=1:3,y=4,training_frame = as.h2o(df1),nfolds = 5)
dnn1
dnn1 = h2o.deeplearning(x=1:3,y=4,training_frame = as.h2o(df1),nfolds = 5, activation = "RectifierWithDropout")
dnn1
dnn1 = h2o.deeplearning(x=1:3,y=4,training_frame = as.h2o(df1),nfolds = 5, activation = "RectifierWithDropout", hidden = c(128,128,128))
dnn1
dnn1 = h2o.deeplearning(x=1:3,y=4,training_frame = as.h2o(df1),nfolds = 5, activation = "RectifierWithDropout", hidden = c(1024))
dnn1
dnn1 = h2o.deeplearning(x=1:3,y=4,training_frame = as.h2o(df1),nfolds = 5, activation = "RectifierWithDropout", hidden = c(1024,1024))
dnn1
dnn1 = h2o.deeplearning(x=1:3,y=4,training_frame = as.h2o(df1),nfolds = 5, activation = "RectifierWithDropout", hidden = c(2048,2048))
dnn1
dnn1 = h2o.deeplearning(x=1:3,y=4,training_frame = as.h2o(df1),nfolds = 5, activation = "RectifierWithDropout", hidden = c(1024,1024),input_dropout_ratio = 0.5)
dnn1
dnn1 = h2o.deeplearning(x=1:3,y=4,training_frame = as.h2o(df1),nfolds = 5, activation = "RectifierWithDropout", hidden = c(1024,1024),l1=1e-5,l2 = 1e-5)
dnn1
dnn1 = h2o.deeplearning(x=1:3,y=4,training_frame = as.h2o(df1),nfolds = 5, activation = "RectifierWithDropout", hidden = c(1024,1024))
dnn1
p3 = h2o.predict(dnn1, as.h2o(df2))
p3
p3 = as.vector(p3$predict)
p3
write.table(p3, file = "predic3.csv", col.names = FALSE, row.names = FALSE)
